<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>Ù…Ø³Ø§Ø± NLP | Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©</title>
<meta name="description" content="Ø¯ÙˆØ±Ø© NLP Ø¹Ù…Ù„ÙŠØ©: Tokenization, Normalization, TF-IDF, POS/NER, Sentiment, Topic Modeling, Embeddings, Transformers. ÙƒÙ„ Ø¯Ø±Ø³ Ø¨ÙƒÙˆØ¯ ÙˆÙ†ØªÙŠØ¬Ø© ÙˆØªÙ…Ø±ÙŠÙ† ÙˆØ±ÙˆØ§Ø¨Ø· ÙÙŠØ¯ÙŠÙˆ.">
<style>
  *,*::before,*::after{box-sizing:border-box}
  :root{
    --bg:#0b1224;--card:#111936;--muted:#93a4c3;--border:#1d2747;--brand:#22d3ee;--brand2:#60a5fa;--accent:#34d399;--text:#e6efff;--text2:#cbd5e1;--code:#0b1021;--codeb:#1f2a44
  }
  body{margin:0;background:radial-gradient(1100px 400px at 100% -10%, rgba(34,211,238,.12), transparent 70%),linear-gradient(180deg,#070b18 0%, #0b1224 50%, #0b1224 100%);color:var(--text);font-family:system-ui,-apple-system,"Cairo","Segoe UI",Tahoma,Arial}
  a{color:var(--brand2);text-decoration:none} a:hover{text-decoration:underline}
  header.hero{border-bottom:1px solid var(--border);padding:40px 22px}
  .hero-inner{max-width:1200px;margin:auto}
  .title{font-size:clamp(26px,5vw,44px);margin:8px 0}
  .subtitle{color:var(--text2);max-width:900px}
  .chips{display:flex;gap:10px;flex-wrap:wrap;margin-top:10px}
  .chip{border:1px dashed var(--border);padding:6px 10px;border-radius:999px;color:var(--text2)}
  .btn{display:inline-flex;align-items:center;gap:8px;border:1px solid var(--border);background:linear-gradient(135deg,var(--brand),var(--brand2));color:#fff;padding:10px 16px;border-radius:10px;font-weight:700;cursor:pointer}
  .btn.secondary{background:transparent}
  .wrap{display:grid;grid-template-columns:290px 1fr;gap:20px;max-width:1200px;margin:auto;padding:22px}
  aside{position:sticky;top:16px;align-self:start;background:var(--card);border:1px solid var(--border);border-radius:14px;padding:14px;height:max-content;max-height:calc(100vh - 32px);overflow:auto}
  .search{display:flex;gap:8px;margin-top:8px}
  .search input{width:100%;padding:10px;border:1px solid var(--border);border-radius:10px;background:transparent;color:var(--text)}
  .progress{height:10px;background:rgba(255,255,255,.06);border:1px solid var(--border);border-radius:999px;overflow:hidden;margin:10px 0}
  .bar{height:100%;width:0%;background:linear-gradient(90deg,var(--accent),var(--brand));transition:width .3s}
  nav a{display:block;padding:8px;border-radius:8px}
  nav a:hover{background:rgba(34,211,238,.1)}
  main{display:grid;gap:16px}
  .card{background:var(--card);border:1px solid var(--border);border-radius:14px;padding:18px}
  h2{margin:4px 0 8px}
  .muted{color:var(--muted)}
  pre{background:var(--code);border:1px solid var(--codeb);color:#d1e9ff;border-radius:12px;padding:14px;overflow:auto;direction:ltr;text-align:left}
  .out{background:#07111f;border:1px dashed #234; color:#9fe89f}
  .code-actions{display:flex;gap:8px;margin-bottom:8px}
  .ghost{background:transparent;border:1px solid var(--border);color:var(--text);padding:6px 10px;border-radius:8px;cursor:pointer}
  .row{display:grid;grid-template-columns:1fr 1fr;gap:14px}
  .note{background:linear-gradient(135deg, rgba(34,211,238,.08), rgba(99,102,241,.08));border:1px dashed var(--border);padding:12px;border-radius:12px}
  .videos a{display:inline-block;margin:6px 8px 0 0;padding:6px 10px;border:1px solid var(--border);border-radius:8px}
  .badge{display:inline-flex;gap:6px;align-items:center;font-size:12px;border:1px solid var(--border);padding:3px 8px;border-radius:999px;color:var(--text2)}
  .exercise{border-top:1px dashed var(--border);margin-top:10px;padding-top:10px}
  @media (max-width:980px){.wrap{grid-template-columns:1fr}.row{grid-template-columns:1fr}}
</style>
</head>
<body>

<header class="hero">
  <div class="hero-inner">
    <div class="badge">ğŸ—£ï¸ Ù…Ø³Ø§Ø± NLP â€¢ Ø¹Ù…Ù„ÙŠ 20 Ø¯Ø±Ø³</div>
    <h1 class="title">NLP: Ù…Ù† Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ø¥Ù„Ù‰ Transformers ÙˆØ§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹</h1>
    <p class="subtitle">ÙƒÙ„ Ø¯Ø±Ø³ ÙŠØ­ØªÙˆÙŠ Ø´Ø±Ø­Ù‹Ø§ØŒ ÙƒÙˆØ¯Ù‹Ø§ Ù‚Ø§Ø¨Ù„Ù‹Ø§ Ù„Ù„ØªØ´ØºÙŠÙ„ØŒ <strong>Ù†ØªÙŠØ¬Ø© Ù…ØªÙˆÙ‚Ø¹Ø©</strong>ØŒ ØªÙ…Ø±ÙŠÙ†Ù‹Ø§ØŒ ÙˆØ±ÙˆØ§Ø¨Ø· ÙÙŠØ¯ÙŠÙˆ (Ø±ÙˆØ§Ø¨Ø· Ø¨Ø­Ø« ÙŠÙˆØªÙŠÙˆØ¨ ÙÙ‚Ø·).</p>
    <div style="margin-top:12px;display:flex;gap:8px;flex-wrap:wrap">
      <button class="btn" id="start">Ø§Ø¨Ø¯Ø£ Ø§Ù„Ø¢Ù†</button>
      <button class="btn secondary" id="dark">Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø¯Ø§ÙƒÙ†/Ø§Ù„ÙØ§ØªØ­</button>
    </div>
    <div class="chips">
      <span class="chip">ğŸ”¤ Tokenization</span>
      <span class="chip">ğŸ§½ Normalization</span>
      <span class="chip">ğŸ§® TF-IDF</span>
      <span class="chip">ğŸ·ï¸ POS / NER</span>
      <span class="chip">ğŸ’¬ Sentiment</span>
      <span class="chip">ğŸ§  Transformers</span>
    </div>
  </div>
</header>

<div class="wrap">
  <aside>
    <strong>ğŸ“š Ø§Ù„Ù…Ø­ØªÙˆÙ‰</strong>
    <div class="search"><input id="q" placeholder="Ø§Ø¨Ø­Ø« Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¯Ø±ÙˆØ³â€¦"></div>
    <div style="display:flex;gap:8px;align-items:center;margin-top:6px"><span class="badge">Ø§Ù„ØªÙ‚Ø¯Ù…: <span id="pct">0%</span></span></div>
    <div class="progress"><div class="bar" id="bar"></div></div>
    <nav id="toc"></nav>
    <div class="note" style="margin-top:10px">ğŸ’¡ Ø´ØºÙ‘Ù„ Ø§Ù„Ø£ÙƒÙˆØ§Ø¯ Ø¯Ø§Ø®Ù„ <span class="badge">Jupyter</span> Ø£Ùˆ Ø£ÙŠ Ù…Ø­Ø±Ø± Python. Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ØªÙ‚Ø±ÙŠØ¨ÙŠØ© ÙˆÙ‚Ø¯ ØªØ®ØªÙ„Ù Ø­Ø³Ø¨ Ø§Ù„Ø¥ØµØ¯Ø§Ø±Ø§Øª.</div>
  </aside>

  <main id="content">
    <!-- L1 -->
    <section class="card lesson" id="l1">
      <h2>1) Ù…Ø§ Ù‡Ùˆ NLP ÙˆÙ„Ù…Ø§Ø°Ø§ Ù‡Ùˆ Ù…Ù‡Ù…ØŸ</h2>
      <p class="muted">Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ØªØ¬Ø¹Ù„ Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ ÙŠÙÙ‡Ù… Ø§Ù„Ù†ØµÙˆØµ: Ø¨Ø­Ø«ØŒ ØªØµÙ†ÙŠÙØŒ Ù…Ø´Ø§Ø¹Ø±ØŒ Ù…Ø­Ø§Ø¯Ø«Ø©ØŒ ØªÙ„Ø®ÙŠØµâ€¦</p>
      <div class="row">
        <div>
          <div class="code-actions"><button class="ghost copy">Ù†Ø³Ø® Ø§Ù„ÙƒÙˆØ¯</button></div>
<pre><code># Ø£Ù…Ø«Ù„Ø© ØªØ·Ø¨ÙŠÙ‚ÙŠØ©
tasks = ["Tokenization","POS","NER","Sentiment","Summarization","QA"]
for i,t in enumerate(tasks,1):
    print(i, t)</code></pre>
        </div>
        <div>
<pre class="out"><code>1 Tokenization
2 POS
3 NER
4 Sentiment
5 Summarization
6 QA</code></pre>
        </div>
      </div>
      <div class="exercise"><strong>ØªÙ…Ø±ÙŠÙ†:</strong> Ø§Ø°ÙƒØ± Ø«Ù„Ø§Ø«Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Øª Ù„Ù†ØµÙˆØµ Ø¹Ø±Ø¨ÙŠØ© ÙÙŠ Ø´Ø±ÙƒØªÙƒ/Ù…Ø´Ø±ÙˆØ¹Ùƒ.</div>
      <div class="videos">
        <a href="https://www.youtube.com/results?search_query=NLP+Ù…Ù‚Ø¯Ù…Ø©" target="_blank">ğŸ¥ Ù…Ù‚Ø¯Ù…Ø© NLP (Ø¹Ø±Ø¨ÙŠ)</a>
        <a href="https://www.youtube.com/results?search_query=natural+language+processing+introduction" target="_blank">ğŸ¥ Intro NLP (EN)</a>
      </div>
      <div class="check"><input type="checkbox" data-done="l1"> <label>ØªÙ…</label></div>
    </section>

    <!-- L2 -->
    <section class="card lesson" id="l2">
      <h2>2) ØªØ«Ø¨ÙŠØª Ø§Ù„Ø¨ÙŠØ¦Ø© ÙˆØ§Ù„Ù…ÙƒØªØ¨Ø§Øª</h2>
      <p class="muted">Ø³Ù†Ø³ØªØ®Ø¯Ù…: <code>regex, nltk, spacy, scikit-learn, gensim, transformers</code></p>
      <div class="row">
        <div>
<pre><code># ÙÙŠ Ø§Ù„Ø·Ø±ÙÙŠØ©:
pip install regex nltk spacy scikit-learn gensim transformers torch
python -m spacy download en_core_web_sm
# Ù†Ù…Ø§Ø°Ø¬ Ø¹Ø±Ø¨ÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ): pip install camel-tools</code></pre>
<pre><code># Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø±ÙŠØ¹
import nltk, spacy, sklearn, gensim, transformers
print("OK")</code></pre>
        </div>
        <div>
<pre class="out"><code>OK</code></pre>
        </div>
      </div>
      <div class="exercise"><strong>ØªÙ…Ø±ÙŠÙ†:</strong> Ø£Ù†Ø´Ø¦ Ø¨ÙŠØ¦Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© ÙˆØ¯ÙˆÙ‘Ù† Ø¥ØµØ¯Ø§Ø±Ø§Øª Ø§Ù„Ø­Ø²Ù… ÙÙŠ requirements.txt.</div>
      <div class="videos">
        <a href="https://www.youtube.com/results?search_query=Ø§Ø¹Ø¯Ø§Ø¯+Ø¨ÙŠØ¦Ø©+Ø¨Ø§ÙŠØ«ÙˆÙ†+NLP" target="_blank">ğŸ¥ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯ (Ø¹Ø±Ø¨ÙŠ)</a>
        <a href="https://www.youtube.com/results?search_query=python+nlp+environment+setup" target="_blank">ğŸ¥ Setup (EN)</a>
      </div>
      <div class="check"><input type="checkbox" data-done="l2"> <label>ØªÙ…</label></div>
    </section>

    <!-- L3 -->
    <section class="card lesson" id="l3">
      <h2>3) ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ (Normalization)</h2>
      <p class="muted">ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø­Ø±ÙˆÙØŒ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ù…ÙˆØ²ØŒ ØªØ®ÙÙŠÙ Ø§Ù„ØªÙƒØ±Ø§Ø±â€¦ Ù…Ù‡Ù… Ù‚Ø¨Ù„ Ø£ÙŠ Ù†Ù…ÙˆØ°Ø¬.</p>
      <div class="row">
        <div>
<pre><code>import re
def normalize_ar(text):
    text = text.strip().lower()
    text = re.sub(r"[Ù‘ÙÙ‹ÙÙŒÙÙÙ’Ù€]", "", text)             # Ø§Ù„ØªØ´ÙƒÙŠÙ„
    text = re.sub(r"[â€œâ€\"'â€™`]", "", text)
    text = re.sub(r"[^\w\s\u0600-\u06FF]", " ", text)  # ØºÙŠØ± Ø§Ù„Ø­Ø±ÙˆÙ
    text = re.sub(r"[Ø§Ø¥Ø¢Ø£]", "Ø§", text).replace("Ù‰","ÙŠ").replace("Ø©","Ù‡")
    text = re.sub(r"\s+", " ", text)
    return text

s = "Ù‡Ù€Ù€Ù€Ø°Ø§Ø§Ø§  Ù†ØµÙ‘ÙŒ ØªØ¬Ø±ÙŠØ¨ÙŠ!! Ø±Ø§Ø¦Ø¹ØŸ"
print(normalize_ar(s))</code></pre>
        </div>
        <div>
<pre class="out"><code>Ù‡Ø°Ø§ Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ Ø±Ø§Ø¦Ø¹</code></pre>
        </div>
      </div>
      <div class="exercise"><strong>ØªÙ…Ø±ÙŠÙ†:</strong> Ø£Ø¶Ù Ø®Ø·ÙˆØ© Ù„Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙˆØ§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ.</div>
      <div class="videos">
        <a href="https://www.youtube.com/results?search_query=ØªÙ†Ø¸ÙŠÙ+Ø§Ù„Ù†ØµÙˆØµ+Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©+NLP" target="_blank">ğŸ¥ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ (Ø¹Ø±Ø¨ÙŠ)</a>
        <a href="https://www.youtube.com/results?search_query=text+normalization+nlp" target="_blank">ğŸ¥ Normalization (EN)</a>
      </div>
      <div class="check"><input type="checkbox" data-done="l3"> <label>ØªÙ…</label></div>
    </section>

    <!-- L4 -->
    <section class="card lesson" id="l4">
      <h2>4) ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª (Tokenization)</h2>
      <div class="row">
        <div>
<pre><code>import re
text = "Ø£Ø­Ø¨ Ø§Ù„Ø¨Ø±Ù…Ø¬Ù‡ØŒ ÙˆØ§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ! NLP Ø±Ø§Ø¦Ø¹."
tokens = re.findall(r"[\u0600-\u06FF]+", text)  # Ø¨Ø³ÙŠØ· Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©
print(tokens)</code></pre>
        </div>
        <div>
<pre class="out"><code>['Ø£Ø­Ø¨', 'Ø§Ù„Ø¨Ø±Ù…Ø¬Ù‡', 'ÙˆØ§Ù„Ø°ÙƒØ§Ø¡', 'Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ', 'Ø±Ø§Ø¦Ø¹']</code></pre>
        </div>
      </div>
      <div class="exercise"><strong>ØªÙ…Ø±ÙŠÙ†:</strong> Ø¬Ø±Ù‘Ø¨ nltk.word_tokenize Ù„Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© ÙˆÙ‚Ø§Ø±Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬.</div>
      <div class="videos">
        <a href="https://www.youtube.com/results?search_query=Tokenization+ar+nlp" target="_blank">ğŸ¥ Tokenization (Ø¹Ø±Ø¨ÙŠ)</a>
        <a href="https://www.youtube.com/results?search_query=tokenization+nlp" target="_blank">ğŸ¥ Tokenization (EN)</a>
      </div>
      <div class="check"><input type="checkbox" data-done="l4"> <label>ØªÙ…</label></div>
    </section>

    <!-- L5 -->
    <section class="card lesson" id="l5">
      <h2>5) Ø¥Ø²Ø§Ù„Ø© ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙˆÙ‚Ù (Stopwords)</h2>
      <div class="row">
        <div>
<pre><code>arabic_stop = {"ÙÙŠ","Ø¹Ù„Ù‰","Ùˆ","Ù…Ù†","Ø¥Ù„Ù‰","Ø¹Ù†","Ù…Ø¹","Ù‡Ø°Ø§","Ù‡Ø°Ù‡","Ø°Ù„Ùƒ","ØªÙ„Ùƒ"}
tokens = ["Ø§Ù†Ø§","Ø§Ø­Ø¨","ØªØ¹Ù„Ù…","Ø§Ù„Ø¨Ø±Ù…Ø¬Ù‡","Ùˆ","Ø§Ù„Ø°ÙƒØ§Ø¡","Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"]
clean = [t for t in tokens if t not in arabic_stop]
print(clean)</code></pre>
        </div>
        <div>
<pre class="out"><code>['Ø§Ù†Ø§', 'Ø§Ø­Ø¨', 'ØªØ¹Ù„Ù…', 'Ø§Ù„Ø¨Ø±Ù…Ø¬Ù‡', 'Ø§Ù„Ø°ÙƒØ§Ø¡', 'Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ']</code></pre>
        </div>
      </div>
      <div class="exercise"><strong>ØªÙ…Ø±ÙŠÙ†:</strong> ÙƒÙˆÙ‘Ù† Ù‚Ø§Ø¦Ù…Ø© Ø¥ÙŠÙ‚Ø§Ù Ù…Ø®ØµÙ‘ØµØ© Ù„Ù…ãƒ‰ÙˆÙ†ØªÙƒ.</div>
      <div class="videos">
        <a href="https://www.youtube.com/results?search_query=stopwords+arabic+nlp" target="_blank">ğŸ¥ Stopwords (Ø¹Ø±Ø¨ÙŠ)</a>
        <a href="https://www.youtube.com/results?search_query=stopwords+nlp" target="_blank">ğŸ¥ Stopwords (EN)</a>
      </div>
      <div class="check"><input type="checkbox" data-done="l5"> <label>ØªÙ…</label></div>
    </section>

    <!-- L6 -->
    <section class="card lesson" id="l6">
      <h2>6) Stemming vs Lemmatization</h2>
      <div class="row">
        <div>
<pre><code># stemming Ø¹Ø±Ø¨ÙŠ Ø¨Ø³ÙŠØ· (Ø¨Ø¯ÙˆÙ† Ù…ÙƒØªØ¨Ø§Øª Ø®Ø§Ø±Ø¬ÙŠØ©)
def light_stem_ar(w):
    for p in ["Ø§Ù„","ÙˆØ§Ù„","Ø¨Ø§Ù„","ÙƒØ§Ù„","ÙØ§Ù„","Ù„Ù„"]:
        if w.startswith(p): w=w[len(p):]
    for s in ["Ù‡","Ù‡Ø§","Ù‡Ù…","Ù‡Ù†","ÙƒÙ…Ø§","ÙƒÙ…Ø§","ÙƒÙ…","ÙƒÙ†","Ù†Ø§","ÙŠ","Ùƒ","Ø§","ÙˆÙ†","ÙŠÙ†","Ø§Ù†","Ø§Øª"]:
        if w.endswith(s): w=w[:-len(s)]
    return w

words = ["ÙˆØ§Ù„Ø¨Ø±Ù…Ø¬Ù‡","Ù…Ø¨Ø±Ù…Ø¬ÙˆÙ†","Ø¨Ø±Ù…Ø¬ÙŠØ§Øª","ÙƒØªØ§Ø¨Ø§ØªÙ‡"]
print([light_stem_ar(w) for w in words])</code></pre>
        </div>
        <div>
<pre class="out"><code>['Ø¨Ø±Ù…Ø¬Ù‡', 'Ù…Ø¨Ø±Ù…Ø¬', 'Ø¨Ø±Ù…Ø¬', 'ÙƒØªØ§Ø¨']   # ØªÙ‚Ø±ÙŠØ¨ÙŠØ©</code></pre>
        </div>
      </div>
      <div class="exercise"><strong>ØªÙ…Ø±ÙŠÙ†:</strong> Ø¬Ø±Ù‘Ø¨ Lemmatization Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… spaCy ÙˆÙ‚Ø§Ø±Ù†.</div>
      <div class="videos">
        <a href="https://www.youtube.com/results?search_query=stemming+lemmatization+arabic" target="_blank">ğŸ¥ Stemming/Lemma (Ø¹Ø±Ø¨ÙŠ)</a>
        <a href="https://www.youtube.com/results?search_query=stemming+vs+lemmatization" target="_blank">ğŸ¥ EN</a>
      </div>
      <div class="check"><input type="checkbox" data-done="l6"> <label>ØªÙ…</label></div>
    </section>

    <!-- L7 -->
    <section class="card lesson" id="l7">
      <h2>7) N-grams ÙˆØ®ØµØ§Ø¦Øµ Ø§Ù„Ù†Øµ</h2>
      <div class="row">
        <div>
<pre><code>from sklearn.feature_extraction.text import CountVectorizer
docs = ["i love nlp","nlp loves python","i love python"]
cv = CountVectorizer(ngram_range=(1,2))
X = cv.fit_transform(docs)
print(cv.get_feature_names_out()[:8])
print(X.toarray())</code></pre>
        </div>
        <div>
<pre class="out"><code>['i', 'i love', 'love', 'love nlp', 'love python', 'nlp', 'nlp loves', 'python']
[[1 1 1 1 0 0 0 0]
 [0 0 0 0 0 1 1 1]
 [1 0 1 0 1 0 0 1]]</code></pre>
        </div>
      </div>
      <div class="exercise"><strong>ØªÙ…Ø±ÙŠÙ†:</strong> Ø¬Ø±Ù‘Ø¨ n-gram Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¹Ù„Ù‰ Ø¬Ù…Ù„ Ù‚ØµÙŠØ±Ø©.</div>
      <div class="videos">
        <a href="https://www.youtube.com/results?search_query=n-grams+Ø´Ø±Ø­" target="_blank">ğŸ¥ N-grams (Ø¹Ø±Ø¨ÙŠ)</a>
        <a href="https://www.youtube.com/results?search_query=n-grams+nlp" target="_blank">ğŸ¥ N-grams (EN)</a>
      </div>
      <div class="check"><input type="checkbox" data-done="l7"> <label>ØªÙ…</label></div>
    </section>

    <!-- L8 -->
    <section class="card lesson" id="l8">
      <h2>8) TF-IDF ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ù†Øµ</h2>
      <div class="row">
        <div>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
docs = ["Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø±Ø§Ø¦Ø¹","Ø§Ù„Ø¨Ø±Ù…Ø¬Ù‡ Ø±Ø§Ø¦Ø¹","Ø§Ù„Ø°ÙƒØ§Ø¡ Ù…ÙÙŠØ¯"]
tfidf = TfidfVectorizer()
X = tfidf.fit_transform(docs)
print(tfidf.get_feature_names_out())
print(X.toarray().round(2))</code></pre>
        </div>
        <div>
<pre class="out"><code>['Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ' 'Ø§Ù„Ø¨Ø±Ù…Ø¬Ù‡' 'Ø§Ù„Ø±Ø§Ø¦Ø¹' 'Ø§Ù„Ø°ÙƒØ§Ø¡' 'Ù…ÙÙŠØ¯']
[[0.71 0.   0.71 0.   0.  ]
 [0.   0.71 0.71 0.   0.  ]
 [0.   0.   0.   0.82 0.58]]   # ØªÙ‚Ø±ÙŠØ¨ÙŠØ©</code></pre>
        </div>
      </div>
      <div class="exercise"><strong>ØªÙ…Ø±ÙŠÙ†:</strong> Ø¬Ø±Ù‘Ø¨ min_df Ùˆmax_df Ù„ØªØµÙÙŠØ© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ø¬Ø¯Ù‹Ø§/Ø§Ù„Ù†Ø§Ø¯Ø±Ø©.</div>
      <div class="videos">
        <a href="https://www.youtube.com/results?search_query=tfidf+Ø´Ø±Ø­" target="_blank">ğŸ¥ TF-IDF (Ø¹Ø±Ø¨ÙŠ)</a>
        <a href="https://www.youtube.com/results?search_query=tfidf+sklearn" target="_blank">ğŸ¥ TF-IDF (EN)</a>
      </div>
      <div class="check"><input type="checkbox" data-done="l8"> <label>ØªÙ…</label></div>
    </section>

    <!-- L9 -->
    <section class="card lesson" id="l9">
      <h2>9) Regex Ø£Ù†Ù…Ø§Ø· Ù†ØµÙŠØ©</h2>
      <div class="row">
        <div>
<pre><code>import re
txt = "ØªÙˆØ§ØµÙ„ Ù…Ø¹Ù†Ø§: support@mail.com Ø£Ùˆ 213-555-1234"
emails = re.findall(r"[\\w.]+@[\\w.]+", txt)
phones = re.findall(r"\\d{3}-\\d{3}-\\d{4}", txt)
print(emails, phones)</code></pre>
        </div>
        <div>
<pre class="out"><code>['support@mail.com'] ['213-555-1234']</code></pre>
        </div>
      </div>
      <div class="exercise"><strong>ØªÙ…Ø±ÙŠÙ†:</strong> Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ù†ØµÙˆØµÙƒ.</div>
      <div class="videos">
        <a href="https://www.youtube.com/results?search_query=Regex+Ø´Ø±Ø­+Ø¹Ø±Ø¨ÙŠ" target="_blank">ğŸ¥ Regex (Ø¹Ø±Ø¨ÙŠ)</a>
        <a href="https://www.youtube.com/results?search_query=regex+python+tutorial" target="_blank">ğŸ¥ Regex (EN)</a>
      </div>
      <div class="check"><input type="checkbox" data-done="l9"> <label>ØªÙ…</label></div>
    </section>

    <!-- L10 -->
    <section class="card lesson" id="l10">
      <h2>10) POS Tagging (spaCy)</h2>
      <div class="row">
        <div>
<pre><code>import spacy
nlp = spacy.load("en_core_web_sm")
doc = nlp("Apple is looking at buying a U.K. startup for $1 billion")
print([(t.text, t.pos_) for t in doc[:7]])</code></pre>
        </div>
        <div>
<pre class="out"><code>[('Apple','PROPN'),('is','AUX'),('looking','VERB'),('at','ADP'),('buying','VERB'),('a','DET'),('U.K.','PROPN')]</code></pre>
        </div>
      </div>
      <div class="exercise"><strong>ØªÙ…Ø±ÙŠÙ†:</strong> Ø¬Ø±Ù‘Ø¨ Ø¬Ù…Ù„Ø© Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ø¹Ø±Ø¨ÙŠ (Ù…Ø«Ù„ CAMeL Tools Ø£Ùˆ spacy-arabic Ø¥Ù† ØªÙˆÙØ±).</div>
      <div class="videos">
        <a href="https://www.youtube.com/results?search_query=POS+Tagging+spaCy+Ø´Ø±Ø­" target="_blank">ğŸ¥ POS (Ø¹Ø±Ø¨ÙŠ)</a>
        <a href="https://www.youtube.com/results?search_query=spacy+pos+tagging" target="_blank">ğŸ¥ POS (EN)</a>
      </div>
      <div class="check"><input type="checkbox" data-done="l10"> <label>ØªÙ…</label></div>
    </section>

    <!-- L11 -->
    <section class="card lesson" id="l11">
      <h2>11) NER Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª</h2>
      <div class="row">
        <div>
<pre><code>import spacy
nlp = spacy.load("en_core_web_sm")
doc = nlp("Elon Musk founded SpaceX in California.")
print([(e.text, e.label_) for e in doc.ents])</code></pre>
        </div>
        <div>
<pre class="out"><code>[('Elon Musk','PERSON'),('SpaceX','ORG'),('California','GPE')]</code></pre>
        </div>
      </div>
      <div class="exercise"><strong>ØªÙ…Ø±ÙŠÙ†:</strong> Ø·Ø¨Ù‘Ù‚ NER Ø¹Ù„Ù‰ Ø£Ø®Ø¨Ø§Ø± ÙˆØ¯ÙˆÙ‘Ù† Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ù‹Ø§.</div>
      <div class="videos">
        <a href="https://www.youtube.com/results?search_query=NER+Ø´Ø±Ø­+spaCy" target="_blank">ğŸ¥ NER (Ø¹Ø±Ø¨ÙŠ)</a>
        <a href="https://www.youtube.com/results?search_query=spacy+ner+tutorial" target="_blank">ğŸ¥ NER (EN)</a>
      </div>
      <div class="check"><input type="checkbox" data-done="l11"> <label>ØªÙ…</label></div>
    </section>

    <!-- L12 -->
    <section class="card lesson" id="l12">
      <h2>12) ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± (Baseline)</h2>
      <div class="row">
        <div>
<pre><code>from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
X = ["i love this","this is bad","amazing product","terrible service","good and nice","not good"]
y = [1,0,1,0,1,0]  # 1=Ø¥ÙŠØ¬Ø§Ø¨ÙŠ
vec = TfidfVectorizer()
Xv = vec.fit_transform(X)
Xtr,Xte,ytr,yte = train_test_split(Xv,y,test_size=.33,random_state=0)
clf = LogisticRegression().fit(Xtr,ytr)
print("Acc:", round(accuracy_score(yte, clf.predict(Xte)),2))</code></pre>
        </div>
        <div>
<pre class="out"><code>Acc: 0.5  # Ø¹ÙŠÙ†Ø© ØµØºÙŠØ±Ø© Ø¬Ø¯Ù‹Ø§Ø› Ø§Ø³ØªØ®Ø¯Ù… Ø¨ÙŠØ§Ù†Ø§Øª Ø£ÙƒØ¨Ø±</code></pre>
        </div>
      </div>
      <div class="exercise"><strong>ØªÙ…Ø±ÙŠÙ†:</strong> Ø²ÙØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ¬Ø±Ø¨ SVM ÙˆLinearSVC.</div>
      <div class="videos">
        <a href="https://www.youtube.com/results?search_query=sentiment+analysis+Ø´Ø±Ø­+Ø¹Ø±Ø¨ÙŠ" target="_blank">ğŸ¥ Sentiment (Ø¹Ø±Ø¨ÙŠ)</a>
        <a href="https://www.youtube.com/results?search_query=sentiment+analysis+sklearn" target="_blank">ğŸ¥ Sentiment (EN)</a>
      </div>
      <div class="check"><input type="checkbox" data-done="l12"> <label>ØªÙ…</label></div>
    </section>

    <!-- L13 -->
    <section class="card lesson" id="l13">
      <h2>13) Topic Modeling (LDA)</h2>
      <div class="row">
        <div>
<pre><code>from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer
docs = ["deep learning with python",
        "nlp with transformers",
        "cooking pasta and pizza",
        "baking cakes and bread"]
cv = CountVectorizer(stop_words="english")
X = cv.fit_transform(docs)
lda = LatentDirichletAllocation(n_components=2, random_state=0).fit(X)
for i,comp in enumerate(lda.components_):
    terms = comp.argsort()[-5:][::-1]
    print("Topic",i, [cv.get_feature_names_out()[t] for t in terms])</code></pre>
        </div>
        <div>
<pre class="out"><code>Topic 0 ['pasta','pizza','cooking','baking','bread']
Topic 1 ['transformers','nlp','python','learning','deep']   # ØªÙ‚Ø±ÙŠØ¨ÙŠØ©</code></pre>
        </div>
      </div>
      <div class="exercise"><strong>ØªÙ…Ø±ÙŠÙ†:</strong> ØºÙŠÙ‘Ø± Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª ÙˆØ¬Ø±Ø¨ Ù†ØµÙˆØµÙ‹Ø§ Ø¹Ø±Ø¨ÙŠØ©.</div>
      <div class="videos">
        <a href="https://www.youtube.com/results?search_query=LDA+Topic+Modeling+Ø¹Ø±Ø¨ÙŠ" target="_blank">ğŸ¥ LDA (Ø¹Ø±Ø¨ÙŠ)</a>
        <a href="https://www.youtube.com/results?search_query=lda+topic+modeling+sklearn" target="_blank">ğŸ¥ LDA (EN)</a>
      </div>
      <div class="check"><input type="checkbox" data-done="l13"> <label>ØªÙ…</label></div>
    </section>

    <!-- L14 -->
    <section class="card lesson" id="l14">
      <h2>14) Word2Vec (Gensim) ØªÙ…Ø«ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª</h2>
      <div class="row">
        <div>
<pre><code>from gensim.models import Word2Vec
sentences = [["i","love","nlp"],["nlp","loves","python"],["i","love","python"]]
w2v = Word2Vec(sentences, vector_size=50, window=3, min_count=1, workers=1, epochs=200)
print(round(float(w2v.wv.similarity("nlp","python")),3))</code></pre>
        </div>
        <div>
<pre class="out"><code>0.62   # ØªÙ‚Ø±ÙŠØ¨ÙŠØ© ÙˆØªØªØºÙŠØ± Ø¨ØªØºÙŠØ± Ø§Ù„ØªØ¯Ø±ÙŠØ¨</code></pre>
        </div>
      </div>
      <div class="exercise"><strong>ØªÙ…Ø±ÙŠÙ†:</strong> Ø¯Ø±Ù‘Ø¨ Ø¹Ù„Ù‰ Ù†ØµÙˆØµ Ø¹Ø±Ø¨ÙŠØ© (Ø£Ø®Ø¨Ø§Ø±/ØªØºØ±ÙŠØ¯Ø§Øª) ÙˆÙ‚ÙØ³ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ÙƒÙ„Ù…Ø§Øª.</div>
      <div class="videos">
        <a href="https://www.youtube.com/results?search_query=word2vec+Ø´Ø±Ø­+Ø¹Ø±Ø¨ÙŠ" target="_blank">ğŸ¥ Word2Vec (Ø¹Ø±Ø¨ÙŠ)</a>
        <a href="https://www.youtube.com/results?search_query=gensim+word2vec+tutorial" target="_blank">ğŸ¥ Gensim (EN)</a>
      </div>
      <div class="check"><input type="checkbox" data-done="l14"> <label>ØªÙ…</label></div>
    </section>

    <!-- L15 -->
    <section class="card lesson" id="l15">
      <h2>15) Doc2Vec / Sentence Embeddings</h2>
      <div class="row">
        <div>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
sents = ["i love nlp","nlp is great","pizza is tasty"]
X = TfidfVectorizer().fit_transform(sents)
sim = cosine_similarity(X)
print(sim.round(2))</code></pre>
        </div>
        <div>
<pre class="out"><code>[[1.   0.53 0.  ]
 [0.53 1.   0.  ]
 [0.   0.   1.  ]]</code></pre>
        </div>
      </div>
      <div class="exercise"><strong>ØªÙ…Ø±ÙŠÙ†:</strong> Ø¬Ø±Ù‘Ø¨ Sentence-Transformers (SBERT) Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Embeddings Ø£ÙØ¶Ù„.</div>
      <div class="videos">
        <a href="https://www.youtube.com/results?search_query=sentence+embeddings+Ø´Ø±Ø­" target="_blank">ğŸ¥ Embeddings (Ø¹Ø±Ø¨ÙŠ)</a>
        <a href="https://www.youtube.com/results?search_query=sentence+transformers+tutorial" target="_blank">ğŸ¥ SBERT (EN)</a>
      </div>
      <div class="check"><input type="checkbox" data-done="l15"> <label>ØªÙ…</label></div>
    </section>

    <!-- L16 -->
    <section class="card lesson" id="l16">
      <h2>16) Ù…Ù‚Ø¯Ù…Ø© Transformers (Hugging Face)</h2>
      <div class="row">
        <div>
<pre><code>from transformers import pipeline
clf = pipeline("sentiment-analysis")
print(clf("I love natural language processing")[0])</code></pre>
        </div>
        <div>
<pre class="out"><code>{'label': 'POSITIVE', 'score': 0.99}   # ØªÙ‚Ø±ÙŠØ¨ÙŠØ©</code></pre>
        </div>
      </div>
      <div class="exercise"><strong>ØªÙ…Ø±ÙŠÙ†:</strong> Ø·Ø¨Ù‘Ù‚ Ø¹Ù„Ù‰ Ù†ØµÙˆØµ Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø§Ø®ØªÙŠØ§Ø± Ù†Ù…ÙˆØ°Ø¬ Ø¹Ø±Ø¨ÙŠ Ù…Ù† ğŸ¤— Hub.</div>
      <div class="videos">
        <a href="https://www.youtube.com/results?search_query=transformers+huggingface+Ø´Ø±Ø­" target="_blank">ğŸ¥ Transformers (Ø¹Ø±Ø¨ÙŠ)</a>
        <a href="https://www.youtube.com/results?search_query=hugging+face+pipeline+tutorial" target="_blank">ğŸ¥ Pipelines (EN)</a>
      </div>
      <div class="check"><input type="checkbox" data-done="l16"> <label>ØªÙ…</label></div>
    </section>

    <!-- L17 -->
    <section class="card lesson" id="l17">
      <h2>17) ØªÙ„Ø®ÙŠØµ Ù†Øµ Ø¨Ø³ÙŠØ· (Summarization)</h2>
      <div class="row">
        <div>
<pre><code>from transformers import pipeline
summ = pipeline("summarization")
text = "Natural Language Processing enables computers to understand human language. It has many applications including search, chatbots and analytics."
print(summ(text, max_length=30, min_length=10, do_sample=False)[0]["summary_text"])</code></pre>
        </div>
        <div>
<pre class="out"><code>"NLP enables computers to understand language with applications in search, chatbots and analytics."   # ØªÙ‚Ø±ÙŠØ¨ÙŠØ©</code></pre>
        </div>
      </div>
      <div class="exercise"><strong>ØªÙ…Ø±ÙŠÙ†:</strong> Ø¬Ø±Ù‘Ø¨ Ù†ØµÙ‹Ø§ Ø£Ø·ÙˆÙ„ ÙˆØ­Ø¯Ø¯ Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ø£Ù‚ØµÙ‰/Ø§Ù„Ø£Ø¯Ù†Ù‰.</div>
      <div class="videos">
        <a href="https://www.youtube.com/results?search_query=text+summarization+arabic" target="_blank">ğŸ¥ Summarization (Ø¹Ø±Ø¨ÙŠ)</a>
        <a href="https://www.youtube.com/results?search_query=huggingface+summarization+pipeline" target="_blank">ğŸ¥ Summarization (EN)</a>
      </div>
      <div class="check"><input type="checkbox" data-done="l17"> <label>ØªÙ…</label></div>
    </section>

    <!-- L18 -->
    <section class="card lesson" id="l18">
      <h2>18) ØªØ±Ø¬Ù…Ø©/Ø£Ø³Ø¦Ù„Ø© ÙˆØ£Ø¬ÙˆØ¨Ø© (Pipelines)</h2>
      <div class="row">
        <div>
<pre><code>from transformers import pipeline
qa = pipeline("question-answering")
context = "Python is a popular programming language created by Guido van Rossum."
print(qa(question="Who created Python?", context=context))</code></pre>
        </div>
        <div>
<pre class="out"><code>{'score': 0.99, 'start': 52, 'end': 68, 'answer': 'Guido van Rossum'}   # ØªÙ‚Ø±ÙŠØ¨ÙŠØ©</code></pre>
        </div>
      </div>
      <div class="exercise"><strong>ØªÙ…Ø±ÙŠÙ†:</strong> Ø¬Ø±Ù‘Ø¨ pipeline Ù„Ù„ØªØ±Ø¬Ù…Ø© "translation" Ø¹Ù„Ù‰ Ø¬Ù…Ù„Ø© Ø¹Ø±Ø¨ÙŠØ©/Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©.</div>
      <div class="videos">
        <a href="https://www.youtube.com/results?search_query=question+answering+transformers+Ø´Ø±Ø­" target="_blank">ğŸ¥ QA (Ø¹Ø±Ø¨ÙŠ)</a>
        <a href="https://www.youtube.com/results?search_query=huggingface+question+answering" target="_blank">ğŸ¥ QA (EN)</a>
      </div>
      <div class="check"><input type="checkbox" data-done="l18"> <label>ØªÙ…</label></div>
    </section>

    <!-- L19 -->
    <section class="card lesson" id="l19">
      <h2>19) Ù…Ø´Ø±ÙˆØ¹: Ù…ØµÙ†Ù‘Ù Ø±Ø³Ø§Ø¦Ù„ (Spam/Not)</h2>
      <div class="row">
        <div>
<pre><code>from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import classification_report
X = ["win cash now","meeting at 5","free prize","project deadline","claim your reward","see you tomorrow"]
y = [1,0,1,0,1,0] # 1=spam
vec = TfidfVectorizer(ngram_range=(1,2))
Xv = vec.fit_transform(X)
Xtr,Xte,ytr,yte = train_test_split(Xv,y,test_size=.33,random_state=42)
clf = SGDClassifier().fit(Xtr,ytr)
pred = clf.predict(Xte)
print(classification_report(yte,pred))</code></pre>
        </div>
        <div>
<pre class="out"><code>precision  recall  f1-score  ...  # ØªÙ‚Ø±ÙŠØ± Ù…Ø¨Ø³Ù‘Ø· (Ø§Ù„Ù‚ÙŠÙ… ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø¹ÙŠÙ†Ø©)</code></pre>
        </div>
      </div>
      <div class="exercise"><strong>ØªÙ…Ø±ÙŠÙ†:</strong> Ø§Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ© (SMS Spam)ØŒ Ù†Ø¸Ù‘ÙÙ‡Ø§ ÙˆØ¯Ø±Ù‘Ø¨ Ù…ÙˆØ¯ÙŠÙ„Ù‹Ø§ Ø£Ù‚ÙˆÙ‰.</div>
      <div class="videos">
        <a href="https://www.youtube.com/results?search_query=sms+spam+classifier+sklearn" target="_blank">ğŸ¥ SMS Spam (EN)</a>
        <a href="https://www.youtube.com/results?search_query=ØªØµÙ†ÙŠÙ+Ø§Ù„Ø±Ø³Ø§Ø¦Ù„+Ø§Ù„Ø¨Ø±ÙŠØ¯+Ø§Ù„Ù…Ø²Ø¹Ø¬+NLP" target="_blank">ğŸ¥ Spam (Ø¹Ø±Ø¨ÙŠ)</a>
      </div>
      <div class="check"><input type="checkbox" data-done="l19"> <label>ØªÙ…</label></div>
    </section>

    <!-- L20 -->
    <section class="card lesson" id="l20">
      <h2>20) Ù†Ø´Ø± Ù†Ù…ÙˆØ°Ø¬ NLP Ø¨Ø³ÙŠØ· (Flask)</h2>
      <p class="muted">Ù†Ø­ÙˆÙ‘Ù„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¥Ù„Ù‰ Ø®Ø¯Ù…Ø© API.</p>
      <div class="row">
        <div>
<pre><code># train_and_save.py
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
import joblib
X=["i love it","bad experience","amazing app","not good"]
y=[1,0,1,0]
vec=TfidfVectorizer().fit(X)
Xv=vec.transform(X)
clf=LogisticRegression().fit(Xv,y)
joblib.dump(vec,"vec.joblib"); joblib.dump(clf,"clf.joblib")</code></pre>
<pre><code># app.py
from flask import Flask, request, jsonify
import joblib
app=Flask(__name__)
vec=joblib.load("vec.joblib"); clf=joblib.load("clf.joblib")
@app.post("/predict")
def predict():
    text=request.json.get("text","")
    X=vec.transform([text]); p=int(clf.predict(X)[0])
    return jsonify({"pred":p})
# ØªØ´ØºÙŠÙ„: flask --app app run</code></pre>
        </div>
        <div>
<pre class="out"><code>POST /predict {"text":"i love this"} â†’ {"pred":1}</code></pre>
        </div>
      </div>
      <div class="exercise"><strong>ØªÙ…Ø±ÙŠÙ† (ØªØ®Ø±Ø¬):</strong> Ø£Ø¶Ù ØªÙˆØ«ÙŠÙ‚ Swagger ÙˆØ­Ø¯ÙˆØ¯ Ù…Ø¹Ø¯Ù„ ÙˆØ§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø³Ø¬Ù„Ø§Øª.</div>
      <div class="videos">
        <a href="https://www.youtube.com/results?search_query=deploy+nlp+model+flask" target="_blank">ğŸ¥ Deploy (EN)</a>
        <a href="https://www.youtube.com/results?search_query=flask+api+Ù†Ø´Ø±+Ù†Ù…ÙˆØ°Ø¬" target="_blank">ğŸ¥ Flask API (Ø¹Ø±Ø¨ÙŠ)</a>
      </div>
      <div class="check"><input type="checkbox" data-done="l20"> <label>ØªÙ…</label></div>
    </section>

  </main>
</div>

<script>
/* Helpers */
const $=s=>document.querySelector(s), $$=s=>Array.from(document.querySelectorAll(s));
const LS={get:k=>JSON.parse(localStorage.getItem(k)||"null"), set:(k,v)=>localStorage.setItem(k,JSON.stringify(v))};

/* Theme */
$("#dark").onclick=()=>document.body.classList.toggle("light");

/* Start */
$("#start").onclick=()=>document.getElementById("l1").scrollIntoView({behavior:"smooth"});

/* TOC + progress + search */
function buildTOC(){
  const toc=$("#toc"); toc.innerHTML="";
  $$("#content .lesson").forEach(sec=>{
    const a=document.createElement("a"); a.textContent=sec.querySelector("h2").textContent; a.href="#"+sec.id; toc.appendChild(a);
  });
}
buildTOC();

const checks=$$("input[type='checkbox'][data-done]");
function updateProgress(){
  const total=checks.length, done=checks.filter(c=>c.checked).length, pct=Math.round(done/total*100)||0;
  $("#bar").style.width=pct+"%"; $("#pct").textContent=pct+"%";
}
checks.forEach(c=>{
  const k="nlp-done:"+c.dataset.done; c.checked=!!LS.get(k);
  c.addEventListener("change",()=>{LS.set(k,c.checked); updateProgress();});
});
updateProgress();

$("#q").addEventListener("input",e=>{
  const q=e.target.value.toLowerCase();
  $$("#content .lesson").forEach(sec=>{
    const ok = sec.textContent.toLowerCase().includes(q);
    sec.style.display = ok ? "" : "none";
  });
});

/* Copy */
document.addEventListener("click",e=>{
  if(e.target.classList.contains("copy")){
    const code=e.target.parentElement.nextElementSibling.innerText;
    navigator.clipboard.writeText(code);
    e.target.textContent="âœ… Ù†ÙØ³Ø®"; setTimeout(()=>e.target.textContent="Ù†Ø³Ø® Ø§Ù„ÙƒÙˆØ¯",1000);
  }
});

/* Smooth anchors */
document.querySelectorAll('a[href^="#"]').forEach(a=>{
  a.addEventListener("click",e=>{
    const id=a.getAttribute("href").substring(1), t=document.getElementById(id);
    if(t){e.preventDefault(); t.scrollIntoView({behavior:"smooth"});}
  });
});
</script>
</body>
</html>
